{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MceD1b9IYte_",
        "outputId": "48382dd2-e6c1-4a44-acc4-66b625c28543"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: mujoco in /usr/local/lib/python3.12/dist-packages (3.3.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from mujoco) (1.4.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.12/dist-packages (from mujoco) (1.13.0)\n",
            "Requirement already satisfied: glfw in /usr/local/lib/python3.12/dist-packages (from mujoco) (2.10.0)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.12/dist-packages (from mujoco) (3.1.10)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.12/dist-packages (from etils[epath]->mujoco) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.12/dist-packages (from etils[epath]->mujoco) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers torch pillow numpy mujoco\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/DorianAtSchool/Franka_table.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkI6yPxqaAYg",
        "outputId": "421a9cfa-2d99-46d4-ee4c-28f8dec1cb4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Franka_table' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
        "from Franka_table.environments.franka_4robots_env import FrankaTable4RobotsEnv"
      ],
      "metadata": {
        "id": "fYMxiF3MY1rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 1. Setup Environment\n",
        "# ============================================================================\n",
        "print(\"Setting up Franka 4-Robots environment...\")\n",
        "env = FrankaTable4RobotsEnv(\n",
        "    mjcf_path=\"../scenes/scene_4robots.xml\",\n",
        "    render_mode=\"rgb_array\"  # Use rgb_array for image capture\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "id": "smI4KDYYY3Rv",
        "outputId": "bc6aec29-f360-4fb0-a4ae-dbcf43168c3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up Franka 4-Robots environment...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ParseXML: Error opening file 'franka_table/scenes/franka_emika_panda/scene_4pandas_table.xml': No such file or directory",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2753135438.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# ============================================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Setting up Franka 4-Robots environment...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m env = FrankaTable4RobotsEnv(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mmjcf_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"../scenes/scene_4robots.xml\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mrender_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"rgb_array\"\u001b[0m  \u001b[0;31m# Use rgb_array for image capture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Franka_table/environments/franka_4robots_env.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, mjcf_path, render_mode, control_dt, physics_dt)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m# Get the directory of this script\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mscript_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mscene_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscript_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"scene_4robots.xml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: ParseXML: Error opening file 'franka_table/scenes/franka_emika_panda/scene_4pandas_table.xml': No such file or directory"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 2. Load OpenVLA Model (GPU Optimized)\n",
        "# ============================================================================\n",
        "print(\"Loading OpenVLA model with GPU optimizations...\")\n",
        "\n",
        "# Check GPU availability\n",
        "if not torch.cuda.is_available():\n",
        "    raise RuntimeError(\"CUDA is not available! Please check your GPU setup.\")\n",
        "\n",
        "device = \"cuda\"\n",
        "print(f\"✓ Using device: {device}\")\n",
        "print(f\"✓ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"✓ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "print(f\"✓ Initial GPU Memory Allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"openvla/openvla-7b\", trust_remote_code=True)\n",
        "\n",
        "vla = AutoModelForVision2Seq.from_pretrained(\n",
        "    \"openvla/openvla-7b\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",  # Automatic GPU placement\n",
        "    low_cpu_mem_usage=True,\n",
        "    trust_remote_code=True,\n",
        "    attn_implementation=\"flash_attention_2\",  # Flash Attention 2 for speed (requires flash-attn package)\n",
        ")\n",
        "\n",
        "# Verify model is on GPU\n",
        "print(f\"\\n✓ Model loaded successfully!\")\n",
        "print(f\"✓ Model device map: {vla.hf_device_map if hasattr(vla, 'hf_device_map') else 'N/A'}\")\n",
        "print(f\"✓ Model dtype: {vla.dtype}\")\n",
        "print(f\"✓ GPU Memory After Loading: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
        "print(f\"✓ GPU Memory Reserved: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")\n",
        "\n",
        "# Test that model parameters are on GPU\n",
        "sample_param = next(vla.parameters())\n",
        "print(f\"✓ Sample parameter device: {sample_param.device}\")\n",
        "print(f\"✓ Sample parameter dtype: {sample_param.dtype}\")\n",
        "\n",
        "if not sample_param.is_cuda:\n",
        "    raise RuntimeError(\"Model parameters are NOT on GPU!\")"
      ],
      "metadata": {
        "id": "OoYwJ9JRY7HM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 3. Helper Functions\n",
        "# ============================================================================\n",
        "def get_robot_state_7d(env, robot_idx=0):\n",
        "    \"\"\"\n",
        "    Extract 7D state (position + quaternion) for a specific robot's end-effector.\n",
        "\n",
        "    Args:\n",
        "        env: FrankaTable4RobotsEnv instance\n",
        "        robot_idx: Which robot (0-3)\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: [x, y, z, qw, qx, qy, qz]\n",
        "    \"\"\"\n",
        "    prefix = env.robot_prefixes[robot_idx]\n",
        "    gripper_site_name = f\"{prefix}gripper_site\"\n",
        "\n",
        "    # Get site ID\n",
        "    import mujoco\n",
        "    gripper_site_id = mujoco.mj_name2id(\n",
        "        env.model,\n",
        "        mujoco.mjtObj.mjOBJ_SITE,\n",
        "        gripper_site_name\n",
        "    )\n",
        "\n",
        "    if gripper_site_id >= 0:\n",
        "        # Get position\n",
        "        position = env.data.site_xpos[gripper_site_id].copy()  # [x, y, z]\n",
        "\n",
        "        # Get orientation (quaternion in MuJoCo format: [w, x, y, z])\n",
        "        quaternion = env.data.site_xquat[gripper_site_id].copy()  # [w, x, y, z]\n",
        "\n",
        "        # Combine into 7D state\n",
        "        state_7d = np.concatenate([position, quaternion])\n",
        "        return state_7d\n",
        "    else:\n",
        "        # Fallback if site not found\n",
        "        return np.zeros(7)\n",
        "\n",
        "def get_observation(env, robot_idx=0, camera_name=None):\n",
        "    \"\"\"\n",
        "    Get RGB image and robot state from environment.\n",
        "\n",
        "    Args:\n",
        "        env: FrankaTable4RobotsEnv instance\n",
        "        robot_idx: Which robot to get state for (0-3)\n",
        "        camera_name: Optional camera name for specific view\n",
        "\n",
        "    Returns:\n",
        "        image: PIL Image\n",
        "        state_7d: np.ndarray of shape (7,) - [x, y, z, qw, qx, qy, qz]\n",
        "    \"\"\"\n",
        "    # Get RGB image\n",
        "    if camera_name:\n",
        "        rgb_array = env.render_camera(camera_name, width=640, height=480)\n",
        "    else:\n",
        "        rgb_array = env.render()\n",
        "\n",
        "    image = Image.fromarray(rgb_array)\n",
        "\n",
        "    # Get 7D state for the specified robot\n",
        "    state_7d = get_robot_state_7d(env, robot_idx)\n",
        "\n",
        "    return image, state_7d\n",
        "\n",
        "def format_prompt(task_description):\n",
        "    \"\"\"Format the prompt for OpenVLA\"\"\"\n",
        "    return f\"In: What action should the robot take to {task_description}?\\nOut:\"\n",
        "\n",
        "def vla_action_to_env_action(vla_action, robot_idx, env):\n",
        "    \"\"\"\n",
        "    Convert VLA 7-DoF action to environment's 32-actuator action.\n",
        "\n",
        "    VLA action: [dx, dy, dz, droll, dpitch, dyaw, gripper]\n",
        "    Env action: 32 values (8 per robot × 4 robots)\n",
        "\n",
        "    This is a simplified mapping - you may need more sophisticated IK.\n",
        "\n",
        "    Args:\n",
        "        vla_action: np.ndarray of shape (7,) from VLA\n",
        "        robot_idx: Which robot this action is for (0-3)\n",
        "        env: FrankaTable4RobotsEnv instance\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray of shape (32,) for environment\n",
        "    \"\"\"\n",
        "    # Start with current control values (hold other robots steady)\n",
        "    full_action = env.data.ctrl.copy()\n",
        "\n",
        "    # Extract components from VLA action\n",
        "    delta_pos = vla_action[:3]  # [dx, dy, dz]\n",
        "    delta_rot = vla_action[3:6]  # [droll, dpitch, dyaw]\n",
        "    gripper = vla_action[6]  # gripper command\n",
        "\n",
        "    # Get current robot joint positions\n",
        "    qpos_start = 7 + robot_idx * 9\n",
        "    current_joints = env.data.qpos[qpos_start:qpos_start+7].copy()\n",
        "\n",
        "    # Simple approach: small joint space movements\n",
        "    # For a real implementation, you'd use inverse kinematics here\n",
        "    # This just applies small deltas to current joint positions\n",
        "    joint_deltas = np.zeros(7)\n",
        "    joint_deltas[0] = delta_pos[0] * 0.1  # Scale down for stability\n",
        "    joint_deltas[1] = delta_pos[1] * 0.1\n",
        "    joint_deltas[2] = delta_pos[2] * 0.1\n",
        "    joint_deltas[3] = delta_rot[0] * 0.1\n",
        "    joint_deltas[4] = delta_rot[1] * 0.1\n",
        "    joint_deltas[5] = delta_rot[2] * 0.1\n",
        "\n",
        "    # Apply to the specific robot's actuators\n",
        "    ctrl_start = robot_idx * 8\n",
        "    full_action[ctrl_start:ctrl_start+7] = current_joints + joint_deltas\n",
        "\n",
        "    # Set gripper (maps -1 to 1 → 0 to 255)\n",
        "    gripper_value = (gripper + 1) * 127.5  # Map [-1, 1] to [0, 255]\n",
        "    full_action[ctrl_start+7] = np.clip(gripper_value, 0, 255)\n",
        "\n",
        "    return full_action"
      ],
      "metadata": {
        "id": "PNRZaD0fY-15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 4. Main Control Loop\n",
        "# ============================================================================\n",
        "def run_episode(task_description=\"move the blue cube to the green goal\",\n",
        "                max_steps=200,\n",
        "                robot_idx=0,\n",
        "                camera_name=None):\n",
        "    \"\"\"\n",
        "    Run one episode with VLA control.\n",
        "\n",
        "    Args:\n",
        "        task_description: Task instruction for VLA\n",
        "        max_steps: Maximum steps per episode\n",
        "        robot_idx: Which robot to control (0-3)\n",
        "        camera_name: Optional specific camera view\n",
        "    \"\"\"\n",
        "    obs, info = env.reset()\n",
        "    prompt = format_prompt(task_description)\n",
        "\n",
        "    print(f\"\\nTask: {task_description}\")\n",
        "    print(f\"Controlling Robot {robot_idx + 1}\")\n",
        "    print(f\"Running for up to {max_steps} steps...\\n\")\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        # Get current observation\n",
        "        image, state_7d = get_observation(env, robot_idx, camera_name)\n",
        "\n",
        "        # Prepare inputs for VLA\n",
        "        inputs = processor(prompt, image).to(device, dtype=torch.bfloat16)\n",
        "\n",
        "        # Verify inputs are on GPU (optional debug check)\n",
        "        # print(f\"Input tensors device: {inputs['pixel_values'].device}\")\n",
        "\n",
        "        # Predict action (7-DoF: delta_pos[3] + delta_rot[3] + gripper[1])\n",
        "        with torch.no_grad():\n",
        "            vla_action = vla.predict_action(\n",
        "                **inputs,\n",
        "                unnorm_key=\"bridge_orig\",\n",
        "                do_sample=False\n",
        "            )\n",
        "\n",
        "        # Verify output is from GPU\n",
        "        if step == 0:\n",
        "            print(f\"✓ VLA output device: {vla_action.device}\")\n",
        "            print(f\"✓ GPU Memory During Inference: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
        "\n",
        "        # Convert to numpy\n",
        "        vla_action = vla_action.cpu().numpy()\n",
        "        if vla_action.ndim > 1:\n",
        "            vla_action = vla_action[0]\n",
        "\n",
        "        # Convert VLA action to full environment action\n",
        "        env_action = vla_action_to_env_action(vla_action, robot_idx, env)\n",
        "\n",
        "        # Step environment\n",
        "        obs, reward, terminated, truncated, info = env.step(env_action)\n",
        "\n",
        "        # Print progress\n",
        "        if step % 20 == 0:\n",
        "            obj_pos = info['object_position']\n",
        "            goal_dist = info['object_to_goal_distance']\n",
        "            print(f\"Step {step:3d}: obj=[{obj_pos[0]:.2f},{obj_pos[1]:.2f},{obj_pos[2]:.2f}], \"\n",
        "                  f\"goal_dist={goal_dist:.3f}, reward={reward:.2f}\")\n",
        "\n",
        "        # Check success\n",
        "        if info.get('success', False):\n",
        "            print(f\"\\n✓ SUCCESS! Object reached goal at step {step}\")\n",
        "            break\n",
        "\n",
        "        # Check if episode is done\n",
        "        if terminated or truncated:\n",
        "            print(f\"\\nEpisode terminated at step {step}\")\n",
        "            break\n",
        "\n",
        "    return step, info"
      ],
      "metadata": {
        "id": "Vqi-o0VlZEI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 5. Multi-Robot Parallel Inference (GPU Accelerated)\n",
        "# ============================================================================\n",
        "def run_episode_multi_robot(task_description=\"move the blue cube to the green goal\",\n",
        "                            max_steps=200,\n",
        "                            robot_indices=[0, 1, 2, 3],\n",
        "                            camera_name=None):\n",
        "    \"\"\"\n",
        "    Run episode controlling multiple robots simultaneously with batched GPU inference.\n",
        "\n",
        "    Args:\n",
        "        task_description: Task instruction for VLA\n",
        "        max_steps: Maximum steps per episode\n",
        "        robot_indices: List of robot indices to control\n",
        "        camera_name: Optional specific camera view\n",
        "    \"\"\"\n",
        "    obs, info = env.reset()\n",
        "    prompt = format_prompt(task_description)\n",
        "\n",
        "    print(f\"\\nTask: {task_description}\")\n",
        "    print(f\"Controlling Robots: {[i+1 for i in robot_indices]}\")\n",
        "    print(f\"Running for up to {max_steps} steps with batched GPU inference...\\n\")\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        # Collect observations for all robots\n",
        "        images = []\n",
        "        states_7d = []\n",
        "        for robot_idx in robot_indices:\n",
        "            image, state_7d = get_observation(env, robot_idx, camera_name)\n",
        "            images.append(image)\n",
        "            states_7d.append(state_7d)\n",
        "\n",
        "        # Batch process all robots on GPU\n",
        "        # Create batch of inputs\n",
        "        batch_inputs = processor(\n",
        "            [prompt] * len(robot_indices),  # Same prompt for all robots\n",
        "            images,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(device, dtype=torch.bfloat16)\n",
        "\n",
        "        # Single batched forward pass on GPU (efficient!)\n",
        "        with torch.no_grad():\n",
        "            vla_actions = vla.predict_action(\n",
        "                **batch_inputs,\n",
        "                unnorm_key=\"bridge_orig\",\n",
        "                do_sample=False\n",
        "            )\n",
        "\n",
        "        # Convert to numpy\n",
        "        vla_actions = vla_actions.cpu().numpy()\n",
        "        if vla_actions.ndim == 1:\n",
        "            vla_actions = vla_actions.reshape(1, -1)\n",
        "\n",
        "        # Start with current control values\n",
        "        full_action = env.data.ctrl.copy()\n",
        "\n",
        "        # Apply each robot's action\n",
        "        for i, robot_idx in enumerate(robot_indices):\n",
        "            vla_action = vla_actions[i]\n",
        "\n",
        "            # Extract components\n",
        "            delta_pos = vla_action[:3]\n",
        "            delta_rot = vla_action[3:6]\n",
        "            gripper = vla_action[6]\n",
        "\n",
        "            # Get current joints\n",
        "            qpos_start = 7 + robot_idx * 9\n",
        "            current_joints = env.data.qpos[qpos_start:qpos_start+7].copy()\n",
        "\n",
        "            # Apply deltas\n",
        "            joint_deltas = np.zeros(7)\n",
        "            joint_deltas[0] = delta_pos[0] * 0.1\n",
        "            joint_deltas[1] = delta_pos[1] * 0.1\n",
        "            joint_deltas[2] = delta_pos[2] * 0.1\n",
        "            joint_deltas[3] = delta_rot[0] * 0.1\n",
        "            joint_deltas[4] = delta_rot[1] * 0.1\n",
        "            joint_deltas[5] = delta_rot[2] * 0.1\n",
        "\n",
        "            # Update action\n",
        "            ctrl_start = robot_idx * 8\n",
        "            full_action[ctrl_start:ctrl_start+7] = current_joints + joint_deltas\n",
        "            full_action[ctrl_start+7] = np.clip((gripper + 1) * 127.5, 0, 255)\n",
        "\n",
        "        # Step environment once with all robot actions\n",
        "        obs, reward, terminated, truncated, info = env.step(full_action)\n",
        "\n",
        "        # Print progress\n",
        "        if step % 20 == 0:\n",
        "            obj_pos = info['object_position']\n",
        "            goal_dist = info['object_to_goal_distance']\n",
        "            gripper_dists = info.get('gripper_distances', [])\n",
        "            avg_gripper = np.mean(gripper_dists) if gripper_dists else 0\n",
        "            print(f\"Step {step:3d}: obj=[{obj_pos[0]:.2f},{obj_pos[1]:.2f},{obj_pos[2]:.2f}], \"\n",
        "                  f\"goal_dist={goal_dist:.3f}, avg_gripper_dist={avg_gripper:.3f}, reward={reward:.2f}\")\n",
        "\n",
        "        # Check success\n",
        "        if info.get('success', False):\n",
        "            print(f\"\\n✓ SUCCESS! Object reached goal at step {step}\")\n",
        "            break\n",
        "\n",
        "        # Check if episode is done\n",
        "        if terminated or truncated:\n",
        "            print(f\"\\nEpisode terminated at step {step}\")\n",
        "            break\n",
        "\n",
        "    return step, info"
      ],
      "metadata": {
        "id": "Q6EutYFdZMzt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}